---
title: "Class 07 - Machine Learning 1"
author: Kristiana Wong A16281367
format: pdf
---

## Clustering Methods

The broad goal here is to find groupings (clusters) in your input data

## Kmeans

First, let's make up some data to cluster. 

```{r}
x <- rnorm(1000)
hist(x)
```

Normally, these distributions values will be around 0, or where ever you set the average to be.

Make a vector of length 60 with 30 points centered at -3 and 30 points at +3.
```{r}
tmp <- c(rnorm(30, mean =-3), rnorm(30, mean=3))
```

I will now make a small x and y dataset with 2 groups of points. 'rev()' function reverses the vector order.

```{r}
rev(c(1:5))
x <- cbind(x= tmp, y = rev(tmp))
x
plot(x)
```

```{r}
k <- kmeans (x, centers = 2)
k
```

Q1. From your result object 'k' how many points are in each cluster?
```{r}
k$size
```

Q2. What "component" of your result object details the cluster membership?
```{r}
k$cluster
```

Q3. Cluster centers?
```{r}
k$centers
```
Q4. Plot of our clustering results
```{r}
plot(x, col = k$cluster)
points(k$centers, col="blue", pch=15, cex=2)
```

```{r}
a <- kmeans(x, centers = 4)
a
```
```{r}
plot(x, col = a$cluster)
points(a$centers, col="blue", pch=15, cex=2)
```
A big limitation is that is does what you ask even if you ask for silly clusters.

## Hierarchical Clustering

The main base R function for Hierarchical Clustering is 'hclust()'. Unlike 'kmeans()' you cannot just pass it your dataset as an input. You first need to calculate a distance matrix.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```
Use plot() to view results
```{r}
plot(hc)
abline(h=10, col="red")
```

To make the "cut" and get our cluster membership vector we can use the 'cutree()' function.

```{r}
grps <- cutree(hc, h=10)
grps
```

Make a plot of our data colored by hclust results.

```{r}
plot(x, col=grps)
```

#Principal Component Analysis (PCA)
Here we will do Principal Component Analysis (PCA for short) on some food data from the UK.
```{r}
url <- "https://tinyurl.com/UK-foods"
UK <- read.csv(url)
UK
```
**Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?**
```{r}
#Code for previewing columns and rows in the UK food dataset
nrow(UK)
ncol(UK)
```

Now we will check our data to make sure that everything is as expected.
```{r}
#Preview the first 6 rows of data of the UK food dataset
head(UK)
```

The rows weren't as we expected, so we should change the columns. Note that the below code though isn't very good. Rather, we should edit the orginial CSV import to have the rows be -1.
```{r}
# Note how the minus indexing works
#rownames(UK) <- UK[,1]
#UK <- UK[,-1]
url <- "https://tinyurl.com/UK-foods"
UK <- read.csv(url, row.names = 1)
UK
head(UK)
```
Now lets check the dimensions again
```{r}
dim(UK)
```

**Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?**

We prefer to edit the CSV file, since the actual edit to the output can manipulate the data to have the incorrect dimensions.

Q3: Changing what optional argument in the above barplot() function results in the following plot?
```{r}
#Changing T -> F changes the plot to the one above
barplot(as.matrix(UK), beside=F, col=rainbow(nrow(UK)))
```

Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The following code generates pairwise plots, pch changes the plotting character. If a given point lies on the diagonal, it means it is being compared to itself.
```{r}
pairs(UK, col=rainbow(10), pch=16)
```

##PCA to the rescue

The main "base" R function for PCA is called 'prcomp()'.

```{r}
pca <- prcomp(t(UK))
summary(pca)
```
Q. How much variance is captured in 2 PCs?
96.5%

To make our main "PC score plot" or PC1 vs PC2 plot (aka "PC1" v.s. "PC2" plot or "PC plot" or "ordination plot". )
```{r}
attributes(pca)
```

We are after the 'pca$x' result component to make our main PCA plot.

```{r}
mycols <- c("orange", "red", "blue", "green")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab="PC1 (67.4%)", ylab="PC2 (29%)")
```

Another important result from PCA is how the original variable (in this case the foods) contribute to the PCs.

This is the contained in the 'pca$rotation' object - this is often called the "loadings" or "contributions" to the PCs.
```{r}
pca$rotation
```
We can make a plot along PC1.
```{r}
library(ggplot2)

contrib <- as.data.frame(pca$rotation)
ggplot(contrib) +
  aes(PC1, rownames(contrib)) +
  geom_col()
```


