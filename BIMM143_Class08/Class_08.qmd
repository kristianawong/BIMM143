---
title: "Class 08"
author: Kristiana Wong A16821367
format: pdf
---
##Mini Project: Unsupervised Leaning with PCA and Clustering

Before we get stuck into project work we will have a quick look at

Read the data from lab 7:
```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names =1)
head(rna.data)
```
Q. How many genes are in this data set?
```{r}
nrow(rna.data)
```
100 rows

##Run PCA
```{r}
pca <- prcomp(t(rna.data), scale = TRUE)
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

```{r}
summary(pca)
```

```{r}
pca$x
```

```{r}
#We have 5 WT and 5 KO samples
mycols <- c(rep("blue", 5), rep("red", 5))
mycols
```

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PCA1", ylab= "PCA2", col= mycols)
```


I could examine which genes contribute most to this first PC 
```{r}
head(sort(abs(pca$rotation[,1]), decreasing = T))
```

#Analysis of Breast Cancer FNA (Fine Needle Aspirations) Data

The data itself comes from the Wisconsin Breast Cnacer Diagnostic Data Set.

Values in this data set describe characteristics of the cell nuclei present in digitzed images of a fine needle aspiration (FNA) of a breast mass.

First, we need to read the data:
```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```

Note that the first column here wisc.df$diagnosis is a pathologist provided expert diagnosis
```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
```

Now I want to make sure I remove that column from my dataset for analysis
```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
head(wisc.data)
```
Q1. How many observations are in this dataset?
```{r}
length(wisc.data)
```

Q2. How many of the observations have a malignant diagnosis?
```{r}
table(wisc.df$diagnosis)
```
212 have malignant diagnosis

Q3. How many variables/features in the data are suffixed with _mean?
```{r}
length(grep("_mean", colnames(wisc.data)))
```

##Principal Component Analysis

Here we will use 'prcomp()' on the 'wisc.data' object - the one without the diagnosis column.

First we have to decide whether to use the 'scale=TRUE' argument when we run 'prcomp()'.

We can look at the means and sd of each column. If they are similar then we are all good to go. If not we should use 'scale = TRUE'.
```{r}
colMeans(wisc.data)
```
```{r}
apply(wisc.data, 2, sd)
```

They are very different so we should scale=TRUE.
```{r}
wisc.pr<- prcomp(wisc.data, scale=T)
summary(wisc.pr)
```
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
3 PCs capture 72.6% of the original variance.


Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7 PCs capture 91.01% of the original variance

##Plotting the PCA results

```{r}
#biplot(wisc.pr)
```
Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
It is extremely difficult to understand, because every point of data is clustered together and you can't see what you are interpreting.

```{r}
attributes(wisc.pr)
```

```{r}
#Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, xlab="PC1", ylab= "PC2")
```
Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis, xlab="PC1", ylab= "PC3")
```
These plots are much easier to interpret and read, and there is much more distinct clustering in these plots than in the dist.plot function.

```{r}
library(ggplot2)
pc <- as.data.frame(wisc.pr$x)
ggplot(pc) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point()
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
wisc.pr$rotation["concave.points_mean",1]
```
-0.2608538 is the component of the loading vector.

Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
```{r}
tbl <- summary(wisc.pr)
which(tbl$importance[3,]>0.8)[1]
```
5 PCs are needed to capture 80% of the variance of the data.

#Hierarchical Clustering

The main function of the Hiearchercal clustering is called 'hclust()', it takes a distance matrix as input.
```{r}
d <- dist(scale(wisc.data))
wisc.hclust <- hclust(d)
plot(wisc.hclust)
```
Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(h=18, col ="red")
grps <- cutree(wisc.hclust, h=18)
table(grps)
grps
```
The height at which the model has 4 clusters is at height 18.

Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
```{r}
better <- cutree(wisc.hclust, h=9)
table(better, diagnosis)
```
Yes, if I cut into 9 I find a better match.

Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
ward.D2 clusters based on the sum of squared differences within all clusters, versus single and complete cluster based on the minimum/maximum distances. Average clusters based on average data. Hence, if we do single/complete, we may not be grouping our data by its actual similarity. Average will not give any meaningful data.

Come back here later to see how our cluster groups correspond to M or B groups.
```{r}
ggplot(pc) +
  aes(PC1, PC2, color = diagnosis) +
  geom_point()
```

##5. Combining methods

Here we will perforom clustering on our PCA results rather than the original data.


In other words we will cluster using 'wisc.pr$x' - our new better variables or PCs. We can choose as many or as few PCs to use as we'd like. It's your call!

```{r}
d.pc <- dist (wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d.pc, method= "ward.D2")
plot(wisc.pr.hclust)
abline(h=80, col ="red")
```
```{r}
grps <- cutree(wisc.pr.hclust, h=80)
table(grps)
```

We can use 'table()' function to make a cross-table as well as just a count table.
```{r}
table(diagnosis)
```

```{r}
table(grps, diagnosis)
```
From these cross-table results, it is indicated that our cluster 1 mostly captures cancer (M) and our cluster 2 mostly captures healthy (B) samples/ individuals.

Q15. How well does the newly created model with four clusters separate out the two diagnoses?

It does very well, and concisely groups the 4 clusters.

Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.
```{r}
wisc.hclust.clusters <- cutree(wisc.pr.hclust, h=35.5)
table(wisc.hclust.clusters, diagnosis)

wisc.km <- kmeans(wisc.data, centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```


Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
```{r}
111/(111+212) #hclust sensitivity
130/(131+212) #kmeans sensitivity

0/(111+357) #hclust specificity
1/(131+357) #kmeans specificity
```
The best sensitivity and specificity was the wisc.kmean.

Q18. Which of these new patients should we prioritize for follow up based on your results?
```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc

plot(wisc.pr$x[,1:2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
Patient 2